\chapter{Fine-grained Cross Modal Retrieval}
\label{cha:Method}

In Chapter \ref{cha:scan} we explained in detail on the structure and principle behind our coarse-grained cross modal retrieval model. According to the result from running artworks datasets on the baseline model, there are aspects that we can focus on to make improvements. In this chapter, we will present our improved model of fine-grained cross modal retrieval.

The structure of this chapter is as follows. Section 4.1 proposes our improved fine-grained cross modal retrieval model on fragment level instead of focusing on a whole image or sentence. Section 4.2 gives a brief explanation on the processed artwork datasets and why these preprocessings are necessary for our model. Section 4.3 shows results of our presented model running on our annotated artworks datasets. Section 4.4 points out some existing shortcomings of our methodology and potential fields to be focused on in the future. Section 4.5 concludes this section.

\section{Fragment Retrieval}
\subsection{Item 1}

\subsubsection{Sub-item 1}

\subsubsection{Sub-item 2}

\subsection{Item 2}

\section{Dataset Preparation}
As we mentioned in Section \ref{cha:intro}, the datasets used in this thesis are from the Rijksmuseum Challenge by Mensink et al. \cite{MensinkICMIR2014}. The raw datasets were downloaded from the webpage dedicated to the Rijksmuseum Challenge. For each artwork, two files were available: a high resolution \verb|jpeg| image file and an accompanying \verb|xml| file including the metadata.
\section{Result}


\section{Future Works}
Besides focusing on the fragment level image/sentence retrieval, it would also be beneficial to consider adopting a new image representation. 

Recently, some works \cite{TranslatingArtworks,parttowhole,Art2Real} have proposed new frameworks for artwork feature extraction and made prominent progress in the field. 


\section{Conclusion}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
