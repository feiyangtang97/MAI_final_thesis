\chapter{Fine-grained Cross Modal Retrieval}
\label{cha:Method}

In Chapter \ref{cha:scan}, we explained in detail on the structure and principle behind our coarse-grained cross-modal retrieval model. As we discussed, there are aspects that we can focus on to make improvements, especially on how to focus on the fine-grained features of artworks. In this chapter, we will present our improved model of fine-grained cross-modal retrieval.

The structure of this chapter is as follows. Section 4.1 gives a brief introduction on our plans for modifying the baseline model into a fragment level. Section 4.2 explains the processed artwork datasets and why the pre-processing is necessary for our model. Section 4.3 proposes our improved fine-grained cross modal retrieval model on fragment level instead of focusing on a whole image or sentence. Section 4.4 shows the results of our presented model running on our annotated artworks datasets. Section 4.5 points out some existing shortcomings of our methodology and potential fields to be focused on in the future. Section 4.6 concludes this section.


\section{Fragment Retrieval}
In this chapter, we are trying to solve the cross-modal retrieval task in a fine-grained level, which means we shall not limit our retrieval on the image or sentence level but a fragment level. We performed some changes on the baseline model in Chapter \ref{cha:scan}:

\begin{itemize}
    \item Instead of attending between image fragments with each token in sentence captions, here we extract noun phrases from sentence captions to perform mutual attention with image fragments.
    \item During the testing process, we replace the original annotations of artworks but adopt manually handcrafted annotations as ground truth, which shall give us a more accurate experiment result.
\end{itemize}

Next, we present our datasets used in this chapter.

\section{Dataset Preparation}
\label{sec:dataprep}
As we mentioned in Section \ref{cha:intro}, the datasets used in this thesis are from the Rijksmuseum Challenge by Mensink et al. \cite{MensinkICMIR2014}. The raw datasets were downloaded from the webpage dedicated to the Rijksmuseum Challenge. For each artwork, two files were available: a high-resolution \verb|jpeg| image file and an accompanying \verb|xml| file including the metadata.

Python scripts (see Appendix \ref{app:A}) were written to parse the \verb|xml| files. Instead of using the raw captions, here the textual attributes we use are already processed into phrases. The technique here used is from Handler et al. \cite{nounphrase}, it extracts noun phrases from captions in raw \verb|xml| files which potentially helped us improve the accuracy as in most cases noun phrases contains more relevant information of artworks.  

As one single artwork can often have more than one annotation existed in its corresponding \verb|xml| file, it is essential to extract all related annotations out and also combine them into one record for training and testing purpose, which saves computational power and simplifies the model input. 

\subsection{Extract Textual Captions for Each Image}

According to those textual captions and their image labels in \verb|xml| files, we can extract the corresponding image names. This extraction process is done using \verb|egyptian_convert.py| in Appendix \ref{app:A}, the sentence captions for each image will have a corresponding \verb|txt| file created. These obtained \verb|txt| captions will then be processed to extract noun phrases in order to achieve the retrieval in a fragment level; results are saved in \verb|json| format. 

Our training tasks will be based on these noun phrases and extracted image fragments. The following section will demonstrate the architecture of our fine-grained cross-modal retrieval model.

\section{Overall Architecture}

The architecture of the fine-grained cross-modal retrieval model is depicted in Figure \ref{fig:mainarch}. This model takes full artwork images and full sentence captions as input. The full artwork image input is a (224, 224) sized colour high-resolution picture and the full sentence caption comes from the Rijksmuseum dataset \cite{MensinkICMIR2014} which contains several descriptions of artworks.

The model has two pipelines processing image and text input from start, for image input, we first detect salient regions as bottom-up attention \cite{bottomup} to extract features from images using faster R-CNN \cite{fasterrcnn} and ResNet-101 \cite{resnet} (mentioned in Section \ref{sec:fasterrcnn}). These obtained image fragments and their representations will be passed into a fully-connected layer, in order to transform them into a joint embedding space with the same dimension of text (i.e. the dimension of GRU explanation in next paragraph).

For full sentence captions as text input, as we mentioned in Section \ref{sec:dataprep}, we do not process the whole sentence directly but first, extract noun phrases out to achieve a finer-grained level. These noun phrases features will be passed into a bidirectional GRU to map them into the same dimension joint embedding space as image features. 

\begin{figure}[h!]
\centering
\includegraphics[width=1.0\textwidth]{archi.pdf}
\caption{Fine-grained Cross Modal Retrieval Model Architecture}
\label{fig:mainarch}
\end{figure}

After we learnt image and text representation in a common space, we can use SCAN to attend image to text and attend text to image, in order to get a better alignment in between. After computed mutual attention between image and text, we can start our follow-up part, which is cross-modal retrieval. We can use our learnt common space representation to calculate similarity scores between image and text; meanwhile, calculate ranked precision and recall for testing.

\section{Experiments}

For experiment settings, we used the same settings and exvironment as mentioned in Chapter \ref{cha:scan}: Ubuntu machine with Intel Xeon Processor E5-1620 (10M Cache, 3.60 GHz) CPU and a GeForce GTX TITAN X GPU. 

\subsubsection{Settings for image representation}

\begin{itemize}
    \item Instead of using the weights pre-trained by \textit{Anderson et al.} on \verb|Visual Genomes| dataset like in Chapter \ref{cha:scan}, here we retrained our faster R-CNN model and ResNet-101 model and performed detection of salient regions as bottom-up attention to extract features from images. 
    \item We captured 36 Region of Interests (ROIs) for each image after average pooling and extracted 2,048-dimensional features vector.
    \item We used L2 normalisation (Euclidean distance) into 1,024 joint embedding spaces (same for GRU), these will be used as image feature vectors.
\end{itemize}

\subsubsection{Settings for text representation}

\begin{itemize}
    \item We obtained 300 dimensional word embedding as input to GRU then use embedding matrix to map it into 1,024 joint embedding spaces.
\end{itemize}

\subsection{Ground Truth}

For this specific task, as we mentioned before, we used our manually annotated ground truth datasets. Each artwork has an updated \verb|xml| file which contains handcrafted image features and textural attributes. Here we show an example of ground truth annotation in Figure \ref{fig:sampledata}.

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\textwidth]{sampledata.pdf}
\caption{Sample Artwork with Ground Truth Features}
\label{fig:sampledata}
\end{figure}

From Figure \ref{fig:sampledata} we are looking at an Egyptian artwork with a cat sculpture. For all object that exist in the artwork, those are, ``\textit{head of a    cat statuette carved in brown, with a pair of standing ears}'' and ``\textit{four dark brown coloured thin shaped legs of a cat sitting in a relaxed posture}'', each of them will have a corresponding detailed location, pose, and truncated information attached. 

\subsection{Evaluation Metrics}

Recall is one of the most commonly used metrics in the field of information retrieval. Here in this research project, we evaluate the performance of sentence retrieval (image query) and image retrieval (sentence query) by the recall at K (R@K). This is defined as the fraction of queries for which the correct item is retrieved in the closest K points to the query. Details of training and the bottom-up attention implementation are explained in Appendix \ref{app:n}.

\subsection{Results}

\begin{table}[h!]
\centering
\begin{tabular}{lcccccc}
                                                                                                  & \multicolumn{3}{c}{Sentence Retrieval}                                       & \multicolumn{3}{c}{Image Retrieval}                                          \\ \hline
Method                                                                                            & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l}{R@10} & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l}{R@10} \\ \hline
Faster R-CNN, ResNet                                                                              & \multicolumn{6}{c}{Ancient Chinese art alignment dataset}                                                                                                  \\ \hline
SCAN t-i LSE ($\lambda_1$ = 9, $\lambda_2$ = 6)                                                                     & 0.1                     & 0.4                     & 0.8                      & 0.1                     & 0.2                     & 0.6                      \\
SCAN t-i AVG ($\lambda_1$ = 9)                                                                             & 0.1                     & 0.1                     & 0.2                      & 0.1                     & 0.2                     & 0.2                      \\
SCAN i-t LSE ($\lambda_1$ = 4, $\lambda_2$ = 20)                                                                    & 0.1                     & 0.2                     & 0.4                      & 0.1                     & 0.1                     & 0.3                      \\
SCAN i-t AVG ($\lambda_1$ = 4)                                                                             & 0.0                     & 0.2                     & 0.5                      & 0.1                     & 0.2                     & 0.3                      \\ \hline
\begin{tabular}[c]{@{}l@{}}SCAN t-i LSE + i-t AVG\\ (with fragmented image and text)\end{tabular} & \textbf{0.2}            & \textbf{1.2}            & \textbf{2.4}             & \textbf{0.2}            & \textbf{1.0}            & \textbf{2.4}            
\end{tabular}
\caption{Result of Fragmented SCAN on Chinese Artwork Dataset}
\label{table:resultfragmentedCN}
\end{table}

%%%%%%%%

\begin{table}[h!]
\centering
\begin{tabular}{lcccccc}
                                                                                                  & \multicolumn{3}{c}{Sentence Retrieval}                                       & \multicolumn{3}{c}{Image Retrieval}                                          \\ \hline
Method                                                                                            & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l}{R@10} & \multicolumn{1}{l}{R@1} & \multicolumn{1}{l}{R@5} & \multicolumn{1}{l}{R@10} \\ \hline
Faster R-CNN, ResNet                                                                              & \multicolumn{6}{c}{Ancient Egyptian art alignment dataset}                                                                                                  \\ \hline
SCAN t-i LSE ($\lambda_1$ = 9, $\lambda_2$ = 6)                                                                     & 0.3                     & 3.4                     & 6.8                      & 0.4                     & 3.2                     & 7.6                      \\
SCAN t-i AVG ($\lambda_1$ = 9)                                                                             & 0.4                     & 3.8                     & 5.2                      & 0.2                     & 3.2                     & 7.2                      \\
SCAN i-t LSE ($\lambda_1$ = 4, $\lambda_2$ = 20)                                                                    & 0.3                     & 3.2                     & 5.4                      & 0.2                     & 2.9                     & 6.3                      \\
SCAN i-t AVG ($\lambda_1$ = 4)                                                                             & 0.3                     & 3.7                     & 6.5                      & 0.3                     & 3.5                     & 8.3                      \\ \hline
\begin{tabular}[c]{@{}l@{}}SCAN t-i LSE + i-t AVG\\ (with fragmented image and text)\end{tabular} & \textbf{0.7}            & \textbf{3.7}            & \textbf{8.1}             & \textbf{0.7}            & \textbf{3.8}            & \textbf{9.6}            
\end{tabular}
\caption{Result of Fragmented SCAN on Egyptian Artwork Dataset}
\label{table:resultfragmentedEG}
\end{table}

Table \ref{table:resultfragmentedCN} and \ref{table:resultfragmentedEG} present the quantitative results on Chinese and Egyptian artwork datasets where all formulations of our proposed method outperform recent approaches in all measures. Here we denote t-i as image retrieval by text and i-t as text retrieval by image, AVG as average pooling and LSE as LogSumExp pooling. Like most of the SCAN based models, we combined t-i and i-t models by averaging their predicted similarity scores. The best result of model ensembles is achieved by combining t-i AVG and i-t LSE, 1.2 on sentence retrieval (R@5) and 1.0 on image retrieval (R@5) relatively on Chinese artwork dataset; 3.7 on sentence retrieval (R@5) and 3.8 on image retrieval (R@5) relatively on Chinese artwork dataset.

We noticed that the recalls for sentence and image retrieval on both Chinese and Egyptian artwork datasets are not satisfactorily high. However, there are still some good retrieval cases which we will demonstrate in the following section.

\subsection{Cross-modal Retrieval Result Examples}
Here we illustrate four typically successful cross-modal retrieval samples of our model. One for each dataset under text retrieval and image retrieval. 

Figure \ref{fig:i2t} shows the text retrieval results from given image queries. The left Egyptian sculpture achieved well captions, the major features such as standing and walking pose, dark brown colour and the man himself were accurately captured. However, as some of the Chinese artworks have significantly different representations and much more fine-grained details, the best case here we had is the following vase. Our textual captions are able to describe its dominant colour and those obvious golden strokes. However, the specific shapes such as flower patterns, thick mouth and big belly challenged the model.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{i2t.pdf}
\caption{Result of Text Retrieval for Given Image Queries}
\label{fig:i2t}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{t2i.pdf}
\caption{Result of Image Retrieval for Given Text Queries
}
\label{fig:t2i}
\end{figure}

Next, Figure \ref{fig:t2i} illustrates the image retrieval results from given text queries. The left Chinese art text query is fairly descriptive and detailed, features like ``bridge'' and ``six cross-patterned circles'' helped our model to pick the right image from other similar ones. The right Egyptian art text queries are fairly easier than the Chinese one as most of the similar artworks differ in shapes and major gestures such as obvious feature ``broken arms raised to the sky''. This potentially helped the model to achieve better results on Egyptian artwork dataset than the Chinese one.

\subsection{Summary of Evaluation}

Generally speaking, Egyptian artworks surpassed Chinese artworks on cross-modal retrieval tasks - extra fine-grained features in Chinese artworks which complicates the process of distinguishing features. Furthermore, low recall results are to be expected. One of the crucial requirements for image-text alignment is accurate feature extraction. However, in our case, we used the bottom-up attention mechanism \cite{bottomup}, which was designed and trained on real-world images. We all know that artworks usually have widely different representations and requires unique and specific treatment on feature extraction. Nevertheless, we did not have more time to redesign our feature extraction algorithm to make it more adaptable to artwork representations. In the next section, we discuss some recently published methodologies on representing artworks, which should be beneficial to improve the current fine-grained cross-modal retrieval model. 

\section{Future Works}

Recently, some works \cite{TranslatingArtworks,parttowhole,Art2Real,tan2017artgan,shen2019discovering} have proposed new frameworks for artwork feature extraction and made good progress in the field. 


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{MTMRoverview.pdf}
\caption{Overview of MTMR Representation Framework\cite{parttowhole}}
\label{fig:mtmroverview}
\end{figure}


Ma el al. \cite{parttowhole} introduced a new methodology for artwork representation in 2017: MTMR (multitask and multi-range). MTMR extracts from the fisher vector based on scale-invariant feature transformation (SIFT) from the painting:

\begin{itemize}
    \item Local, regional and global features
    \item Multiclass area coding structure
    \item Multitask learning framework
\end{itemize}

Figure \ref{fig:mtmroverview} illustrates the overall architecture of the MTMR representation framework. The above, middle and bottom represent different levels of feature extraction using SIFT-based fisher vector: local, regional and global, respectively. Inside each level, there are multiclass area coding and multitask learning structures. These obtained features in different levels will be finally passed to the random forest model as an ensemble mechanism to generate a final result.

Figure \ref{fig:mtmrmulti} shows the detailed structure of how multitask learning works in the MTMR framework. The right grey box contains several different tasks which are split to process in parallel. There is a residual network inside the green box which consists of fully connected layers and convolutional layers for artwork image feature extraction.


\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{MTMRmultitask.pdf}
\caption{MTMR's Multitask Learning Structure\cite{parttowhole}}
\label{fig:mtmrmulti}
\end{figure}


Besides the MTMR framework, in 2019, Shen et al. \cite{shen2019discovering} tried to find almost repetitive patterns from a large number of artworks. 

Due to the differences in artistic media (oil paintings, pastel paintings, sketches, etc.) and the inherent deviations in the reproduction process, this goal is more difficult to mine than standard examples. The critical technology is to use self-supervised learning to fine-tune standard depth features on specific art collections to adapt them to this task. Correctly, use spatial consistency between adjacent feature matches as a supervised fine-tuning signal. The adjusted features enable more accurate matching (not affected by differences in style) and can be used with standard pattern discovery methods based on geometric verification to identify repeating patterns in the dataset.

Figure \ref{fig:featurelearning} demonstrates the feature learning process proposed. First candidate correspondences need to be obtained, which come from matching the proposal regions in red boxes with the original complete dataset. Next, these correspondences are checked by comparing features from validation regions (in blue). Finally, only positive results from previous step will be extracted and kept. 

The method is evaluated on multiple different datasets and showed significant qualitative findings. In terms of quantitative evaluation, the researchers marked 273 approximately repeating details in the dataset of 1587 works of art by Lao Jan Bruegel and his studio. In addition to works of art, the researchers also showed improvements in the positioning of the algorithm on the Oxford5K photo dataset and historical photo positioning on the LTLL (Large Time Lags Location) dataset.

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{featurelearningartwork.pdf}
\caption{Feature Learning Strategy\cite{shen2019discovering}}
\label{fig:featurelearning}
\end{figure}

We believe that adopting a comprehensive framework like MTMR \cite{parttowhole} along with a fine-grained feature matching and extraction process mentioned in Shen et al.'s research \cite{shen2019discovering} will significantly help improve the fine-grained cross modal retrieval model.

\section{Conclusion}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
