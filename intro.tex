\chapter{Introduction}
\label{cha:intro}

%\epigraph{\textit{``If we want machines to think, we need to teach them to see.''}}{\textit{- Li Fei-Fei}}

%% shurong: replace this part (til the end of 1.1) by some introductions on image-text alignment and cross modal retrieval

As a part of the millions of internet users who often surf on the internet, we always ``Google'': enter the keywords to be searched to retrieve our desired text information, which is to retrieve text with text in this case. Sometimes we also upload images on Google to find similar ones, which refers to using an image to retrieve images. However, considering the situation when we use textual information to retrieve images on Google, at this time the type of information we enter and the type of information obtained is different, known as ``cross modal''.

Cross modal retrieval can be understood as finding the relationship between different modal samples and using a particular modal sample to search for other modal samples with approximate semantics. For example: use the image to retrieve the corresponding text, or use the text to retrieve the desired image. Of course, modals are not limited to images and text, such as voice, physiological signals, and video can be used as components of cross modal retrieval.

The goal of cross modal retrieval is to calculate the similarity between different modal data. For a given query sample, retrieve different modal data related to the query sample. The key challenge lies in the inconsistent representation of different modalities, making it difficult to directly measure the similarity, that is, the ``semantic gap'' problem. There are two mainstream cross-modal retrieval methods: common space learning methods and cross-modal similarity measurement methods.

The common space learning method enables a cross-modal similarity to be directly measured in this space by learning a unified common space for different modal data, mainly including traditional statistical association analysis methods, deep learning based methods, graph-based reduction Methods, metric learning methods, ranking learning methods, dictionary learning methods, cross modal hashing methods, etc.

The cross modal similarity measurement method does not learn the common space, but directly calculates the cross modal similarity, mainly including graph-based methods and nearest neighbour analysis methods. Besides, there are some other cross modal retrieval methods, such as correlation feedback analysis method, multi-modal theme model method, etc.

At present, the types of multimedia in cross modal retrieval mainly include images, text, voice, video and 3D graphics. Most cross modal retrieval methods are currently limited to the use of images and text, as well as a small number of other types of voice and video. On the one hand, almost no databases are containing these five modal types; on the other hand, there are ``semantic gaps'' in the various forms of different modal types.

The ``semantic gap'' problem is a core challenge faced by cross-modal retrieval: data of different modalities have different feature representations, and their similarity is difficult to measure directly. In order to solve the above problem, an intuitive method is to unify the representation across modalities, that is, to map different modal data from their independent representation spaces into a third-party, public space so that they can measure the similarity of each other. In recent years, with the rapid development and broad application of deep learning, the unified representation method based on deep learning has become a research hot spot and mainstream.

%As the most basic and influential part of human's sensory nervous system, vision plays a vital part in all kinds of recognition tasks. A glance at an image is sufficient for a human to point out and describe an immense amount of details about the visual scene. Traditionally, object recognition and description were performed manually. As time passed, the number of images in many systems grew to more substantial than terabyte size, and could no longer be maintained manually, which brought the idea of machine learning and computer vision also formed a part of artificial intelligence. 

%The process of ``teaching machines to see'' has been widely discussed since the late 20$^{th}$ century and focused on modelling the 3D shape of objects. The latter focus emphasises the identification of objects, and this was achieved by adopting the AdaBoost \cite{adaboost} algorithm. Recently, since neural network-based algorithm AlexNet \cite{alexnet} won the object recognition ImageNet \cite{imagenet} competition in 2012, the field of computer vision began to make breakthroughs. At present, the top technology in the field of computer vision has been continuously approaching human performance. Besides, the learning mechanism, performance and security of neural networks are discussed in more depth.

%\section{Computer Vision}

%Computer vision (CV) refers to the ability of machines to perceive the environment, and it is the subject of researching machine vision capabilities, or the subject that enables machines to analyse the environment and the stimuli therein visually. Machine vision usually involves the evaluation of images or videos. The British Machine Vision Association (BMVA) defines machine vision as "the automatic extraction, analysis, and understanding of useful information from a single image or a series of images." \cite{bmva}

%A real understanding of our environment is not achievable only by visual representation. More precisely, it is the process of transmitting visual cues to the primary visual cortex through the optic nerve, and then being analysed by the brain in a highly characterised form. Extracting explanations from this sensory information contains almost all of our natural evolution and subject experience, that is, how evolution survives us, and how we learn and understand the world in our lifetime.

%In this respect, the visual process is only the process of transmitting images and interpreting them. However, from a computational point of view, the images are closer to thought or cognition and involve many functions of the brain. Therefore, due to the remarkable cross-domain characteristics, many people think that computer vision is a real understanding of the visual environment and its context, and will lead us to achieve reliable artificial intelligence.

\section{Motivation}
%% shurong: extend the motivation in the abstract and emphasize that image-text alignment in the cultural heritage domain has not been a lot exploit yet, they are quite popular for natural images. you can also merge this part with the above part if the content is too less
Image-text alignment is a fundamental research topic in the inter-field of computer vision and natural language processing. It includes two subtasks: image annotation and image search. 



It can save the intense labour from annotating the artworks for online
digital artwork archives if we can automatically describe an artwork image or sub-image with its textual attributes. Furthermore, this topic can help to boost the multi-modal question answering performance in the cultural heritage domain by providing fine-grained image-text correspondence information \cite{mqa}. Therefore, it is interesting to explore the methods that can figure out the artwork image or sub-image and text correspondence.

While a large number of papers discussed aligning image-text and coarse-grained modal information retrieval, the fragment level image-text alignment problem has not been as widely dealt within the multi-modal question answering research domain. Coarse-grained modal information retrieval can retrieve information between image and text. However, it usually does not work well on artwork datasets which usually contain some fine-grained patterns and objects in one artwork image, therefore reduce the effectiveness of the retrieval model. In this section, we look at two simple examples of ancient Egyptian artworks and how textual description can be generated to match its image.

\begin{figure}[h!]
\centering
\includegraphics[width=0.35\textwidth]{artwork_fine1.pdf}
\caption{Ancient Egyptian Artwork Example (coarse-grained)}
\label{fig:artwork1}
\end{figure}

Figure \ref{fig:artwork1} shows a small blue container with black inked motifs on it. Using a coarse-grained multi-modal retrieval model, we can generate the textual description ``\textit{painted black inscriptions on a bright blue background}'', which is sufficient enough for this artwork. However, in the real-world scenarios, there is much more likely for us to encounter an artwork showing in Figure \ref{fig:artwork2}. Our traditional coarse-grained multi-modal retrieval model generates ``\textit{a red and a white pot}'' for this artwork but it is not detailed and did not cover sufficient information in the artwork image. Therefore, this motivated us to propose a fine-grained multi-modal retrieval model which can focus on the fragment level image/sentence retrieval. The description on the image was generated by our fine-grained multi-modal retrieval model, which has significantly more detailed information.

\begin{figure}[h!]
\centering
\includegraphics[width=0.45\textwidth]{artwork_fine2.pdf}
\caption{Ancient Egyptian Artwork Example (fine-grained)}
\label{fig:artwork2}
\end{figure}


%% shurong: move the examples in 1.2 to the dataset part. There are also redundant info in this section. double check and remove repeated content
\section{Dataset}

The datasets \cite{artworkcaption} involved in this thesis research are collected from the following online sources: the Brooklyn Museum \cite{brooklynmuseum}, the Metropolitan Museum \cite{themet}, and the British Museum \cite{thebritishmuseum}. Based on these sources, we have created two artwork datasets: the ancient Egyptian art image dataset and the ancient Chinese art image dataset. The two datasets are collected based on the geographical location of the origin of the artworks because caption words may differ much depending on the cultural background of the location. Detailed statistics of the two datasets are shown in Table \ref{fig:datasetstats}. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset}          & \textbf{Num. of Artworks} & \textbf{Aver. Length} & \textbf{Num. of Tokens} \\ \hline
\textbf{Egyptian} & 16,146                       & 9                       & 10,694                     \\ \hline
\textbf{Chinese}  & 6,847                        & 10                      & 4,721                      \\ \hline
\end{tabular}
\caption{Statistics of Our Datasets \cite{artworkcaption}}
\label{fig:datasetstats}
\end{table}

The datasets have 22,993 high-quality artwork images recorded
in a controlled setting. Images are stored at varying dpi and the
compressed \verb|jpeg| image file size ranges between 20-300 KB. The paragraph-level descriptions are split into multiple sentences and a maximum of five sentences are retained for each artwork to reduce data imbalance. In addition, we removed noisy texts from the captions following a specific pattern, e.g, ``See 13.26.59'' \cite{artworkcaption}. The number is the accession number of an artifact that obviously cannot be derived from the input image or artwork type. We also remove duplicate images in the captioning datasets based on their hash code. Tokens occurring less than two times are removed from the training vocabulary. The datasets are all split into an 80\%, 10\%, and 10\% partition for respectively training, validation, and test.

Each artwork image has a corresponding record saved in a \verb|json| file containing their processed caption textual data. However, in our research, instead of using the original captions in the \verb|json| file, we extracted noun phrases from them and performed our alignment tasks between these noun phrases and images.

Here we focus on ancient Egyptian and Chinese artworks; we manually construct our datasets; they consist of 16,146 images from Egyptian domain and 6,847 images in Chinese. Figure \ref{fig:sampleEgyptian} and Figure \ref{fig:sampleChinese} show three examples of artworks from the Egyptian and Chinese collection with textual captions.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{egyptian.pdf}
\caption{Examples of Artworks of Egyptian Artwork Dataset}
\label{fig:sampleEgyptian}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{chinese.pdf}
\caption{Examples of Artworks of Chinese Artwork Dataset}
\label{fig:sampleChinese}
\end{figure}

\section{Research Questions}

In this thesis, we study fine-grained image-text alignment for artworks. Fine-grained image-text alignment refers to the fragment level cross modal (between image and text) retrieval. The prior sections list the background and motivations, the specific research questions that we look at are:

%% two research questions: 1. is the alignment model experimented on natural images effective for artwork items 2. can coarse-grained cross-modal retrieval modal be adapted to fine-grained retrieval and how?
\begin{itemize}
    \item Why mainstream coarse-grained image-text alignment techniques does not suit artwork domains well?
    \item Can we propose a practical approach that facilitates the textual attributes annotation of artwork images accurately and efficiently?
    \item Can we develop a fine-grained image-text alignment technique that can retrieve text from images and vice versa on fragment level?
\end{itemize}

\subsection{Cross Modal Retrieval Framework}

As mentioned above, our primary research task here is to achieve cross modal retrieval (i.e. between image and text) for artworks. Figure \ref{fig:framework} illustrates a brief working framework for the tasks.

%% [solved]shurong: the training data is a bit confusing. an image should be composed of a certain number of image fragments and the text is noun-phrase composed sentence .Or the picture can be kept here but you explain the two levels of retrieval clearly or even give examples.
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{framework.pdf}
\caption{Cross Modal Retrieval Process}
\label{fig:framework}
\end{figure}

We train our multi-modal machine learning model on the known image features and corresponding textual attributes; this training process helps us learn the potential relationships between image and text. This trained model will be used to generate textual attributes from known image features and vice versa. We believe this may help with automating the artwork annotation process and significantly save labours on artworks classifications and searches.


\section{Contributions}
The main contributions made by this thesis are:

\begin{enumerate}
    \item We review work from several disciplines which may be of relevance to the present subject of inquiry and provide commentary on how the findings from these disciplines may be useful. (Section~\ref{cha:relatedworks})
    \item We adopted SCAN \cite{scan} as the coarse-grained cross modal retrieval model, analysed its structure and applied it on our proposed Egyptian and Chinese artworks datasets to achieve the image-text alignment. By employing this model, we are able to have a coarse-grained alignment between artworks image and textual attributes. This lays the foundation of our improved fine-grained model. (Section~\ref{cha:scan})
    \item By focusing on fragment level image features and textual attributes instead of feeding the whole images and sentences, we are able to perform cross modal retrieval in a more fine-grained level. This allows the future multi-modal retrieval tasks on artworks to achieve more accurate and stable results. (Section~\ref{cha:Method})
\end{enumerate}


\section{Structure of Thesis}

This thesis is structured into the following chapters:

\begin{itemize}
    
    \item \textit{Section~\ref{cha:intro} Introduction}\newline
    We provide the reader with a relevant background to understand this thesis.

    \item \textit{Section~\ref{cha:relatedworks} Related Works}\newline
    We introduce relevant research in image recognition, deep learning, object detection, natural language processing and image-text alignment. In particular, we detail seminal research and review the overall state of the current research. We also review the difference in the works pertaining to the traditional visual-semantic alignment technique versus the more recent cross attention image-text alignment framework.
    
    \item \textit{Section~\ref{cha:scan} Coarse-grained Cross Modal Retrieval}\newline
    We introduce our coarse-grained cross-modal retrieval modal - SCAN, discuss how its components interact with each other and explain how SCAN uses cross attention to improve image-text alignment. We also show the preliminary result running SCAN on our ancient Egyptian and Chinese artwork datasets.
    
    \item \textit{Section~\ref{cha:Method} Fine-grained Cross Modal Retrieval}\newline
    We proposed our improved fine-grained cross modal retrieval model, which now focus more on the fragment level image-text alignment. We then perform several experiments on evaluating the effectiveness of our image generation from text and vice versa by the recall. We also point out the direction of possible future improvements by discussing several recent related publications.
    
    \item \textit{Section~\ref{cha:conclusion} Conclusion}\newline
    We conclude the work and add some final reflections and remarks.
\end{itemize}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
