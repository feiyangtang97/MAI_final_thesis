\chapter{Implementation Details}
\label{app:A}
This appendix provides some \verb|Python| source code for the implementation mentioned in Section \ref{cha:Method}.

\section{Project Structure}
In this section, we go through the structure of my implementation using structured tables. This gives the audience and future researchers a brief idea on how the process looks like.


\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{Source Code File} & \multicolumn{1}{c|}{Description}  \\ \hline
\verb|egyptian_convert.py|             & \begin{tabular}[c]{@{}l@{}}Contains \verb|xml| parser and \verb|json| parser. \\ This generates caption for each picture, \\ extract features base on the corresponding \\ caption and also generate features for each \\ image by using parsed \verb|xml| file\end{tabular} \\ \hline
\verb|preprocess.ipynb|                 & \begin{tabular}[c]{@{}l@{}}Combines the features extracted from \\ botton-up attention (Egyptian)\end{tabular}                                                                                                                                          \\ \hline
\verb|chinese_artworks_convert.ipynb| & \begin{tabular}[c]{@{}l@{}}Combines the features extracted from \\ botton-up attention (Chinese)\end{tabular}                                                                                                                                           \\ \hline
\end{tabular}
\label{table:preprocesspython}
\caption{Python Source Code Files for Preprocessing}
\end{table}


%%%%%%%%%%%%%%

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{Source Code File} & \multicolumn{1}{c|}{Description}                                                                                                                             \\ \hline
\verb|data.py|                                & \begin{tabular}[c]{@{}l@{}}Class \verb|PrecompDataset| is where we changed\\ the path of extracted phrases and also where\\ to change process methods.\end{tabular} \\ \hline
\verb|model.py|                               & Provides SCAN model based on VSE++.                                                                                                                           \\ \hline
\verb|train.py|                               & \begin{tabular}[c]{@{}l@{}}Provides training process using settings in\\ \verb|model.py|.\end{tabular}                                                               \\ \hline
\end{tabular}
\label{table:trainpython}
\caption{Python Source Code Files for Training}
\end{table}



%%%%%%%%%%%%%%%

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{Source Code File} & \multicolumn{1}{c|}{Description}                                                                                          \\ \hline
\verb|evaluation.py|                          & \begin{tabular}[c]{@{}l@{}}Provides testing process using the fragment\\ level annotations and ground truth.\end{tabular} \\ \hline
\verb|demo.ipynb|                             & Provides a demo for testing process.                                                                                      \\ \hline
\end{tabular}
\label{table:testpython}
\caption{Python Source Code Files for Testing}
\end{table}


\subsection{Obtain Image Features}
We used bottom-up attention \cite{bottomup} to extract features from our Egyptian and Chinese artwork images. This methodolgy used a faster R-CNN and a ResNet101 as core architecture. We obtained a pre-trained model from its \href{https://github.com/peteanderson80/bottom-up-attention}{Github} open repository which was trained on \verb|VisualGenome| dataset consisting a large amount of real-world images. A sample image feature extraction command for Egyptian training images is displayed below:

\begin{lstlisting}
$ python bottom-up-features/extract_features.py 
--image_dir artworks/train --out_dir artworks/features 
--cfg bottom-up-features/cfgs/faster_rcnn_resnet101.yml 
--model bottom-up-features/models/bottomup_pretrained_10_100.pth
\end{lstlisting}

We save these obtained image features under \verb|/features| directory. These image features of Egyptian and Chinese artworks are available in numpy array format, which can be used for training directly in the future.

\subsection{Obtain Textual Features}

According to captions and corresponding image labels, we can extract the corresponding image names from \verb|xml| and \verb|json| files which contains textual attributes. This extraction process can be done using \verb|egyptian_convert.py|, \verb|python| script then generates captions for each image, make sure that each has a corresponding \verb|.txt| file created. These produced \verb|.txt| files are saved in \verb|/phrase| directory. 

To achieve the retrieval on a fragment level, we then extract noun phrases from these \verb|.txt| files. We use \verb|/preprocess.ipynb| to obtain \verb|vocab.json|. The API adopted here was proposed by Handler et al. \cite{nounphrase} and available on \href{https://github.com/slanglab/phrasemachine}{Github} as well.

\subsection{Training and Testing}
For training process, we can simply modify the \verb|PrecompDataset| class in \verb|data.py| to change related processing methods. A sample training command for Chinese training images is displayed below (with i-t average pooling formulation):

\begin{lstlisting}
python train.py --data_name chinese_artworks 
--logger_name $RUN_PATH/chinese_artworks_scan/log 
--model_name $RUN_PATH/chinese_artworks_scan/log 
--max_violation --bi_gru --img_dim 2048
--agg_func=Mean --cross_attn=i2t --lambda_softmax=4
\end{lstlisting}

For testing process, we load our saved model then run \verb|evaluation.py| script. A sample evaluation command is shown below:

\begin{lstlisting}[language=python]
from vocab import Vocabulary
import evaluation
evaluation.evalrank("$RUN_PATH/chinese_artworks_scan/model_best.pth.tar", data_path="$DATA_PATH", split="test")
\end{lstlisting}


%%%%%%%%%%%%%%

\section{Python Snippets}
In this section, we briefly display the preprocessing part of our implementation which is also crucial to the entire project as an efficient and accuracy extraction from \verb|xml| files can make the training and testing process easier and more smooth.

Here we focus on two specific files, one is \verb|egyptian_convert.py| which is able to parse \verb|xml| and \verb|json| files. Another one is \verb|preprocess.ipynb| which combines features and vocabularies for each distinct artwork.


\subsection{Texual Attribute Parser}

Here the code snippet of \verb|egyptian_convert.py| is displayed below. Similarly, \verb|chinese_artwork_convert.py| works for Chinese artwork dataset.

\begin{lstlisting}[language=Python]
import xml.etree.ElementTree as ET
import os
import numpy as np
from tqdm import tqdm
import json

# this module used to generate caption for each picture and extract features base on the corresponding caption

# parse the xml and get the text for relevant element

def get_text(path):
    tree = ET.parse(path)
    root = tree.getroot()
    file_name = root[1].text
    res = []
    for object in tree.findall("object"):
        res.append(object[0].text)
    return file_name,res

# generate features for each image by using parsed xml file

def xml_parser(path,features_path,save_path):
    list_dir = os.listdir(path)
    features = []
    not_list = []
    for image in tqdm(list_dir):
        file_path = os.path.join(features_path, image.split('.')[0] + '.npy')
        feature = np.load(file_path, allow_pickle=True).tolist()["features"]
        print(feature)
        feature = np.array(feature)
        features.append(feature[:10, :])
    features = np.stack(features)
    np.save(save_path, features)
    
def json_parser(path):

    # load the json files extract the sentence and corresponding picture name.
    
    with open(path,"r") as file:
        data = json.load(file)
        for image in data["images"]:
            try:
                file_name = image["filename"]+".txt"
                sentence = image["sentences"]
                sentence = sentence[0]["raw"]
            except:
                continue
            # save the sentence in relevant files
            with open(os.path.join("../data/phrase_train", file_name),"w") as f:
                f.write(sentence)
                
\end{lstlisting}

\subsection{Feature Combination}
Here the code snippet of \verb|preprocess.ipynb| is displayed below.

\begin{lstlisting}[language=Python]
import torchvision
import torch
import json
import cv2
import os.path as osp
from PIL import Image
from tqdm import tqdm_notebook
import numpy as np
import nltk
from collections import Counter

# combine the features extracted from botton-up attention

content = json.load(open(osp.join(dataset_name, 'caption_data.json')))

images = {
    'train': [],
    'test': [],
    'val': []
}

for image in content['images']:
    filepath = image['filepath']
    if len(image['sentences']) == 0:
        continue
    images[filepath].append(image)
    
for phase in images.keys():
    features = []
    for image in tqdm_notebook(images[phase]):
        file_path = osp.join(dataset_name, 'features', image['filename'].split('.')[0] + '.npy')
        print(file_path)
        feature = np.load(file_path)
        features.append(feature[:10, :])
    features = np.stack(features)
    np.save(osp.join(dataset_name, phase), features)

from vocab import Vocabulary, serialize_vocab
from collections import Counter
import phrasemachine

threshold = 4
counter = {}
for phase in images.keys():
    captions = [x['sentences'][0]['raw'] for x in images[phase]]
    for caption in captions:
        temp = list(phrasemachine.get_phrases(caption)["counts"].keys())
        for key in temp:
            if key not in counter.keys():
                counter[key] = 1
            else:
                counter[key] += 1
# Discard if the occurrence of the word is less than min_word_cnt.
words = [word for word, cnt in counter.items() if cnt >= threshold]

# create a vocab wrapper and add some special tokens
vocab = Vocabulary()
vocab.add_word('<pad>')
vocab.add_word('<start>')
vocab.add_word('<end>')
vocab.add_word('<unk>')

# Add words to the vocabulary.
for i, word in enumerate(words):
    vocab.add_word(word)
serialize_vocab(vocab, osp.join(dataset_name, 'vocab.json'))

with open(osp.join(dataset_name, 'data.json'), 'w') as f:
    json.dump(images, f)
\end{lstlisting}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
